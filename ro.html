<!DOCTYPE html>
<html>
<head>
  <title>buttons</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      padding: 40px;
      text-align: center;
    }
    .copy-btn {
      width: 140px;
      height: 40px;
      margin: 10px;
      font-size: 16px;
      background-color: #3498db;
      color: white;
      border: none;
      border-radius: 6px;
      cursor: pointer;
    }
    #status {
      margin-top: 20px;
      color: green;
      font-weight: bold;
    }
  </style>
</head>
<body>

  <h2>BB</h2>

  <!-- Buttons with hidden code snippets -->
  <button class="copy-btn" onclick="copyHiddenCode(this)" data-code="import pandas as pd&#10;import numpy as np&#10;import seaborn as sns&#10;import matplotlib.pyplot as plt&#10;from sklearn.datasets import fetch_california_housing&#10;&#10;# Step 1: Load the California Housing dataset&#10;data = fetch_california_housing(as_frame=True)&#10;housing_df = data.frame&#10;&#10;# Step 2: Create histograms for numerical features&#10;numerical_features = housing_df.select_dtypes(include=[np.number]).columns&#10;&#10;# Plot histograms&#10;plt.figure(figsize=(15, 10))&#10;for i, feature in enumerate(numerical_features):&#10;    plt.subplot(3, 3, i + 1)&#10;    sns.histplot(housing_df[feature], kde=True, bins=30, color='blue')&#10;    plt.title(f'Distribution of {feature}')&#10;plt.tight_layout()&#10;plt.show()&#10;&#10;# Step 3: Generate box plots for numerical features&#10;plt.figure(figsize=(15, 10))&#10;for i, feature in enumerate(numerical_features):&#10;    plt.subplot(3, 3, i + 1)&#10;    sns.boxplot(x=housing_df[feature], color='orange')&#10;    plt.title(f'Box Plot of {feature}')&#10;plt.tight_layout()&#10;plt.show()&#10;&#10;# Step 4: Identify outliers using the IQR method&#10;print(&quot;Outliers Detection:&quot;)&#10;outliers_summary = {}&#10;for feature in numerical_features:&#10;    Q1 = housing_df[feature].quantile(0.25)&#10;    Q3 = housing_df[feature].quantile(0.75)&#10;    IQR = Q3 - Q1&#10;    lower_bound = Q1 - 1.5 * IQR&#10;    upper_bound = Q3 + 1.5 * IQR&#10;    outliers = housing_df[(housing_df[feature] &lt; lower_bound) | (housing_df[feature] &gt; upper_bound)]&#10;    outliers_summary[feature] = len(outliers)&#10;    print(f&quot;{feature}: {len(outliers)} outliers&quot;)&#10;&#10;# Optional: Print a summary of the dataset&#10;print(&quot;\nDataset Summary:&quot;)&#10;print(housing_df.describe())">r1</button>
  <button class="copy-btn" onclick="copyHiddenCode(this)" data-code="import pandas as pd&#10;import seaborn as sns&#10;import matplotlib.pyplot as plt&#10;from sklearn.datasets import fetch_california_housing&#10;&#10;# Step 1: Load the California Housing Dataset&#10;california_data = fetch_california_housing(as_frame=True)&#10;data = california_data.frame&#10;&#10;# Step 2: Compute the correlation matrix&#10;correlation_matrix = data.corr()&#10;&#10;# Step 3: Visualize the correlation matrix using a heatmap&#10;plt.figure(figsize=(10, 8))&#10;sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)&#10;plt.title('Correlation Matrix of California Housing Features')&#10;plt.show()&#10;&#10;# Step 4: Create a pair plot to visualize pairwise relationships&#10;sns.pairplot(data, diag_kind='kde', plot_kws={'alpha': 0.5})&#10;plt.suptitle('Pair Plot of California Housing Features', y=1.02)&#10;plt.show()">r2</button>
  <button class="copy-btn" onclick="copyHiddenCode(this)" data-code="import numpy as np&#10;import pandas as pd&#10;from sklearn.datasets import load_iris&#10;from sklearn.decomposition import PCA&#10;import matplotlib.pyplot as plt&#10;&#10;# Load the Iris dataset&#10;iris = load_iris()&#10;data = iris.data&#10;labels = iris.target&#10;label_names = iris.target_names&#10;&#10;# Convert to a DataFrame for better visualization&#10;iris_df = pd.DataFrame(data, columns=iris.feature_names)&#10;&#10;# Perform PCA to reduce dimensionality to 2&#10;pca = PCA(n_components=2)&#10;data_reduced = pca.fit_transform(data)&#10;&#10;# Create a DataFrame for the reduced data&#10;reduced_df = pd.DataFrame(data_reduced, columns=['Principal Component 1', 'Principal Component 2'])&#10;reduced_df['Label'] = labels&#10;&#10;# Plot the reduced data&#10;plt.figure(figsize=(8, 6))&#10;colors = ['r', 'g', 'b']&#10;for i, label in enumerate(np.unique(labels)):&#10;    plt.scatter(&#10;        reduced_df[reduced_df['Label'] == label]['Principal Component 1'],&#10;        reduced_df[reduced_df['Label'] == label]['Principal Component 2'],&#10;        label=label_names[label],&#10;        color=colors[i]&#10;    )&#10;&#10;plt.title('PCA on Iris Dataset')&#10;plt.xlabel('Principal Component 1')&#10;plt.ylabel('Principal Component 2')&#10;plt.legend()&#10;plt.grid()&#10;plt.show()">r3</button>
  <button class="copy-btn" onclick="copyHiddenCode(this)" data-code="import pandas as pd&#10;&#10;def find_s_algorithm(file_path):&#10;    data = pd.read_csv(file_path)&#10;&#10;    print('Training data:')&#10;    print(data)&#10;&#10;    attributes = data.columns[:-1]&#10;    class_label = data.columns[-1]&#10;&#10;    hypothesis = ['?' for _ in attributes]&#10;&#10;    for index, row in data.iterrows():&#10;        if row[class_label] == 'Yes':&#10;            for i, value in enumerate(row[attributes]):&#10;                if hypothesis[i] == '?' or hypothesis[i] == value:&#10;                    hypothesis[i] = value&#10;                else:&#10;                    hypothesis[i] = '?'&#10;&#10;    return hypothesis&#10;&#10;file_path = 'training_data.csv'&#10;hypothesis = find_s_algorithm(file_path)&#10;print(&quot;&#92;nThe final hypothesis is:&quot;, hypothesis)">r4</button>
  <button class="copy-btn" onclick="copyHiddenCode(this)" data-code="import numpy as np&#10;import matplotlib.pyplot as plt&#10;from collections import Counter&#10;&#10;data = np.random.rand(100)&#10;&#10;labels = [&quot;Class1&quot; if x &lt;= 0.5 else &quot;Class2&quot; for x in data[:50]]&#10;&#10;def euclidean_distance(x1, x2):&#10;    return abs(x1 - x2)&#10;&#10;def knn_classifier(train_data, train_labels, test_point, k):&#10;    distances = [(euclidean_distance(test_point, train_data[i]), train_labels[i]) for i in range(len(train_data))]&#10;&#10;    distances.sort(key=lambda x: x[0])&#10;    k_nearest_neighbors = distances[:k]&#10;&#10;    k_nearest_labels = [label for _, label in k_nearest_neighbors]&#10;&#10;    return Counter(k_nearest_labels).most_common(1)[0][0]&#10;&#10;train_data = data[:50]&#10;train_labels = labels&#10;&#10;test_data = data[50:]&#10;&#10;k_values = [1, 2, 3, 4, 5, 20, 30]&#10;&#10;print(&quot;--- k-Nearest Neighbors Classification ---&quot;)&#10;print(&quot;Training dataset: First 50 points labeled based on the rule (x &lt;= 0.5 -&gt; Class1, x &gt; 0.5 -&gt; Class2)&quot;)&#10;print(&quot;Testing dataset: Remaining 50 points to be classified\n&quot;)&#10;&#10;results = {}&#10;&#10;for k in k_values:&#10;    print(f&quot;Results for k = {k}:&quot;)&#10;    classified_labels = [knn_classifier(train_data, train_labels, test_point, k) for test_point in test_data]&#10;    results[k] = classified_labels&#10;&#10;    for i, label in enumerate(classified_labels, start=51):&#10;        print(f&quot;Point x{i} (value: {test_data[i - 51]:.4f}) is classified as {label}&quot;)&#10;    print(&quot;\n&quot;)&#10;&#10;print(&quot;Classification complete.\n&quot;)&#10;&#10;for k in k_values:&#10;    classified_labels = results[k]&#10;    class1_points = [test_data[i] for i in range(len(test_data)) if classified_labels[i] == &quot;Class1&quot;]&#10;    class2_points = [test_data[i] for i in range(len(test_data)) if classified_labels[i] == &quot;Class2&quot;]&#10;&#10;    plt.figure(figsize=(10, 6))&#10;    plt.scatter(train_data, [0] * len(train_data), c=[&quot;blue&quot; if label == &quot;Class1&quot; else &quot;red&quot; for label in train_labels],&#10;                label=&quot;Training Data&quot;, marker=&quot;o&quot;)&#10;    plt.scatter(class1_points, [1] * len(class1_points), c=&quot;blue&quot;, label=&quot;Class1 (Test)&quot;, marker=&quot;x&quot;)&#10;    plt.scatter(class2_points, [1] * len(class2_points), c=&quot;red&quot;, label=&quot;Class2 (Test)&quot;, marker=&quot;x&quot;)&#10;&#10;    plt.title(f&quot;k-NN Classification Results for k = {k}&quot;)&#10;    plt.xlabel(&quot;Data Points&quot;)&#10;    plt.ylabel(&quot;Classification Level&quot;)&#10;    plt.legend()&#10;    plt.grid(True)&#10;    plt.show()">
r5
</button>
  <button class="copy-btn" onclick="copyHiddenCode(this)" data-code="import numpy as np&#10;import matplotlib.pyplot as plt&#10;&#10;&#10;def gaussian_kernel(x, xi, tau):&#10;    return np.exp(-np.sum((x - xi) ** 2) / (2 * tau ** 2))&#10;&#10;def locally_weighted_regression(x, X, y, tau):&#10;    m = X.shape[0]&#10;    weights = np.array([gaussian_kernel(x, X[i], tau) for i in range(m)])&#10;    W = np.diag(weights)&#10;    X_transpose_W = X.T @ W&#10;    theta = np.linalg.inv(X_transpose_W @ X) @ X_transpose_W @ y&#10;    return x @ theta&#10;&#10;np.random.seed(42)&#10;X = np.linspace(0, 2 * np.pi, 100)&#10;y = np.sin(X) + 0.1 * np.random.randn(100)&#10;X_bias = np.c_[np.ones(X.shape), X]&#10;&#10;x_test = np.linspace(0, 2 * np.pi, 200)&#10;x_test_bias = np.c_[np.ones(x_test.shape), x_test]&#10;tau = 0.5&#10;y_pred = np.array([locally_weighted_regression(xi, X_bias, y, tau) for xi in x_test_bias])&#10;&#10;plt.figure(figsize=(10, 6))&#10;plt.scatter(X, y, color='red', label='Training Data', alpha=0.7)&#10;plt.plot(x_test, y_pred, color='blue', label=f'LWR Fit (tau={tau})', linewidth=2)&#10;plt.xlabel('X', fontsize=12)&#10;plt.ylabel('y', fontsize=12)&#10;plt.title('Locally Weighted Regression', fontsize=14)&#10;plt.legend(fontsize=10)&#10;plt.grid(alpha=0.3)&#10;plt.show()">
r6
</button>
<button class="copy-btn" onclick="copyHiddenCode(this)" data-code="import numpy as np;&#10;import pandas as pd;&#10;import matplotlib.pyplot as plt;&#10;from sklearn.datasets import fetch_california_housing;&#10;from sklearn.model_selection import train_test_split;&#10;from sklearn.linear_model import LinearRegression;&#10;from sklearn.preprocessing import PolynomialFeatures, StandardScaler;&#10;from sklearn.pipeline import make_pipeline;&#10;from sklearn.metrics import mean_squared_error, r2_score;&#10;&#10;def linear_regression_california():&#10;    housing = fetch_california_housing(as_frame=True)&#10;    X = housing.data[[&quot;AveRooms&quot;]] &#10;    y = housing.target &#10;&#10;    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)&#10;&#10;    model = LinearRegression()&#10;    model.fit(X_train, y_train)&#10;&#10;    y_pred = model.predict(X_test)&#10;&#10;    plt.scatter(X_test, y_test, color=&quot;blue&quot;, label=&quot;Actual&quot;)&#10;    plt.plot(X_test, y_pred, color=&quot;red&quot;, label=&quot;Predicted&quot;)&#10;    plt.xlabel(&quot;Average number of rooms (AveRooms)&quot;)&#10;    plt.ylabel(&quot;Median value of homes ($100,000)&quot;)&#10;    plt.title(&quot;Linear Regression - California Housing Dataset&quot;)&#10;    plt.legend()&#10;    plt.show()&#10;&#10;    print(&quot;Linear Regression - California Housing Dataset&quot;)&#10;    print(&quot;Mean Squared Error:&quot;, mean_squared_error(y_test, y_pred))&#10;    print(&quot;R^2 Score:&quot;, r2_score(y_test, y_pred))&#10;&#10;def polynomial_regression_auto_mpg():&#10;    url = &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data&quot;&#10;    column_names = [&quot;mpg&quot;, &quot;cylinders&quot;, &quot;displacement&quot;, &quot;horsepower&quot;, &quot;weight&quot;, &quot;acceleration&quot;, &quot;model_year&quot;, &quot;origin&quot;]&#10;    data = pd.read_csv(url, sep='\\s+', names=column_names, na_values=&quot;?&quot;)&#10;    data = data.dropna()&#10;&#10;    X = data[&quot;displacement&quot;].values.reshape(-1, 1) &#10;    y = data[&quot;mpg&quot;].values&#10;&#10;    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)&#10;&#10;    poly_model = make_pipeline(PolynomialFeatures(degree=2), StandardScaler(), LinearRegression())&#10;    poly_model.fit(X_train, y_train)&#10;&#10;    y_pred = poly_model.predict(X_test)&#10;&#10;    plt.scatter(X_test, y_test, color=&quot;blue&quot;, label=&quot;Actual&quot;)&#10;    plt.scatter(X_test, y_pred, color=&quot;red&quot;, label=&quot;Predicted&quot;)&#10;    plt.xlabel(&quot;Displacement&quot;)&#10;    plt.ylabel(&quot;Miles per gallon (mpg)&quot;)&#10;    plt.title(&quot;Polynomial Regression - Auto MPG Dataset&quot;)&#10;    plt.legend()&#10;    plt.show()&#10;&#10;    print(&quot;Polynomial Regression - Auto MPG Dataset&quot;)&#10;    print(&quot;Mean Squared Error:&quot;, mean_squared_error(y_test, y_pred))&#10;    print(&quot;R^2 Score:&quot;, r2_score(y_test, y_pred))&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    print(&quot;Demonstrating Linear Regression and Polynomial Regression\n&quot;)&#10;    linear_regression_california()&#10;    polynomial_regression_auto_mpg();">r7</button>

  <button class="copy-btn" onclick="copyHiddenCode(this)" data-code="# Importing necessary libraries&#10;import numpy as np&#10;import matplotlib.pyplot as plt&#10;from sklearn.datasets import load_breast_cancer&#10;from sklearn.model_selection import train_test_split&#10;from sklearn.tree import DecisionTreeClassifier&#10;from sklearn.metrics import accuracy_score&#10;from sklearn import tree&#10;&#10;data = load_breast_cancer()&#10;X = data.data&#10;y = data.target&#10;&#10;X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)&#10;clf = DecisionTreeClassifier(random_state=42)&#10;clf.fit(X_train, y_train)&#10;y_pred = clf.predict(X_test)&#10;&#10;accuracy = accuracy_score(y_test, y_pred)&#10;print(f&quot;Model Accuracy: {accuracy * 100:.2f}%&quot;)&#10;new_sample = np.array([X_test[0]])&#10;prediction = clf.predict(new_sample)&#10;&#10;prediction_class = &quot;Benign&quot; if prediction == 1 else &quot;Malignant&quot;&#10;print(f&quot;Predicted Class for the new sample: {prediction_class}&quot;)&#10;&#10;plt.figure(figsize=(12,8))&#10;tree.plot_tree(clf, filled=True, feature_names=data.feature_names, class_names=data.target_names)&#10;plt.title(&quot;Decision Tree - Breast Cancer Dataset&quot;)&#10;plt.show();">r8</button>
  <button class="copy-btn" onclick="copyHiddenCode(this)" data-code="import numpy as np;&#10;from sklearn.datasets import fetch_olivetti_faces;&#10;from sklearn.model_selection import train_test_split, cross_val_score;&#10;from sklearn.naive_bayes import GaussianNB;&#10;from sklearn.metrics import accuracy_score, classification_report, confusion_matrix;&#10;import matplotlib.pyplot as plt;&#10;&#10;data = fetch_olivetti_faces(shuffle=True, random_state=42)&#10;X = data.data&#10;y = data.target;&#10;&#10;X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)&#10;&#10;gnb = GaussianNB()&#10;gnb.fit(X_train, y_train)&#10;y_pred = gnb.predict(X_test)&#10;&#10;accuracy = accuracy_score(y_test, y_pred)&#10;print(f'Accuracy: {accuracy * 100:.2f}%')&#10;&#10;print(&quot;\nClassification Report:&quot;)&#10;print(classification_report(y_test, y_pred, zero_division=1))&#10;&#10;print(&quot;\nConfusion Matrix:&quot;)&#10;print(confusion_matrix(y_test, y_pred))&#10;&#10;cross_val_accuracy = cross_val_score(gnb, X, y, cv=5, scoring='accuracy')&#10;print(f'\nCross-validation accuracy: {cross_val_accuracy.mean() * 100:.2f}%')&#10;&#10;fig, axes = plt.subplots(3, 5, figsize=(12, 8))&#10;for ax, image, label, prediction in zip(axes.ravel(), X_test, y_test, y_pred):&#10;    ax.imshow(image.reshape(64, 64), cmap=plt.cm.gray)&#10;    ax.set_title(f&quot;True: {label}, Pred: {prediction}&quot;)&#10;    ax.axis('off')&#10;&#10;plt.show();">r9</button>
 <button class="copy-btn" onclick="copyHiddenCode(this)" data-code="import numpy as np;&#10;import pandas as pd;&#10;import matplotlib.pyplot as plt;&#10;import seaborn as sns;&#10;from sklearn.datasets import load_breast_cancer;&#10;from sklearn.cluster import KMeans;&#10;from sklearn.preprocessing import StandardScaler;&#10;from sklearn.decomposition import PCA;&#10;from sklearn.metrics import confusion_matrix, classification_report;&#10;&#10;data = load_breast_cancer()&#10;X = data.data&#10;y = data.target&#10;&#10;scaler = StandardScaler()&#10;X_scaled = scaler.fit_transform(X)&#10;&#10;kmeans = KMeans(n_clusters=2, random_state=42)&#10;y_kmeans = kmeans.fit_predict(X_scaled)&#10;&#10;print(&quot;Confusion Matrix:&quot;)&#10;print(confusion_matrix(y, y_kmeans))&#10;print(&quot;&#10;Classification Report:&quot;)&#10;print(classification_report(y, y_kmeans))&#10;&#10;pca = PCA(n_components=2)&#10;X_pca = pca.fit_transform(X_scaled)&#10;&#10;df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])&#10;df['Cluster'] = y_kmeans&#10;df['True Label'] = y&#10;&#10;plt.figure(figsize=(8, 6))&#10;sns.scatterplot(data=df, x='PC1', y='PC2', hue='Cluster', palette='Set1', s=100, edgecolor='black', alpha=0.7)&#10;plt.title('K-Means Clustering of Breast Cancer Dataset')&#10;plt.xlabel('Principal Component 1')&#10;plt.ylabel('Principal Component 2')&#10;plt.legend(title=&quot;Cluster&quot;)&#10;plt.show()&#10;&#10;plt.figure(figsize=(8, 6))&#10;sns.scatterplot(data=df, x='PC1', y='PC2', hue='True Label', palette='coolwarm', s=100, edgecolor='black', alpha=0.7)&#10;plt.title('True Labels of Breast Cancer Dataset')&#10;plt.xlabel('Principal Component 1')&#10;plt.ylabel('Principal Component 2')&#10;plt.legend(title=&quot;True Label&quot;)&#10;plt.show()&#10;&#10;plt.figure(figsize=(8, 6))&#10;sns.scatterplot(data=df, x='PC1', y='PC2', hue='Cluster', palette='Set1', s=100, edgecolor='black', alpha=0.7)&#10;centers = pca.transform(kmeans.cluster_centers_)&#10;plt.scatter(centers[:, 0], centers[:, 1], s=200, c='red', marker='X', label='Centroids')&#10;plt.title('K-Means Clustering with Centroids')&#10;plt.xlabel('Principal Component 1')&#10;plt.ylabel('Principal Component 2')&#10;plt.legend(title=&quot;Cluster&quot;)&#10;plt.show();">r10</button>


  <p id="status"></p>

  <script>
    function copyHiddenCode(button) {
      const code = button.getAttribute("data-code");
      navigator.clipboard.writeText(code).then(() => {
        document.getElementById("status").innerText = "....";
      }).catch(() => {
        document.getElementById("status").innerText = "0000";
      });
    }
  </script>

</body>
</html>
